{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# from src.dataloader.meta_loader import Dataloader4SER, NShotMAMLSampler\n",
    "from src.audio.audio import AudioProcessor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataloader for SER in meta-learning approach\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# First we define a global dataloader which simply loads\n",
    "# the pairs of wav files and emotion labels\n",
    "class Dataloader4SER(data.Dataset):\n",
    "    '''\n",
    "        Given a csv file with wav_path and label columns,\n",
    "        it returns pairs of raw wav_file path and labels.\n",
    "    \n",
    "        TODO: Process directly a disered audio transformation\n",
    "        (instead of pass the raw wav path pass the melspectrogram or the mfccs)\n",
    "    '''\n",
    "    def __init__(self, df_path, ap, pad_to=100, pad_value = 0):\n",
    "        self.df_path = df_path # Must have wav_path and labels columns\n",
    "        self.ap = ap\n",
    "        self.pad_to = pad_to\n",
    "        self.pad_value = pad_value\n",
    "        \n",
    "        self.all_data = pd.read_csv(self.df_path) # Must be comma delimited\n",
    "\n",
    "        cols = self.all_data.columns\n",
    "        if('wav_path' not in cols):\n",
    "            raise RuntimeError('There is no > wav_path < column.')\n",
    "\n",
    "        if('emotion' not in cols):\n",
    "            raise RuntimeError('There is no > emotion < column.')\n",
    "\n",
    "        # if('language' not in cols):\n",
    "        #     raise RuntimeError('There is no > language < column.')\n",
    "\n",
    "        # If dataset is complete then get the lists \n",
    "        self.x = self.all_data['wav_path'].to_list()\n",
    "        self.y = self.all_data['emotion'].to_list()\n",
    "        # self.l = self.all_data['language'].to_list()\n",
    "\n",
    "    def load_wav(self, filename):\n",
    "        audio = self.ap.load_wav(filename)\n",
    "        return audio\n",
    "\n",
    "    def load_data(self, index):\n",
    "        wav = self.x[index]\n",
    "        emotion = self.y[index]\n",
    "\n",
    "        # w = np.asarray(self.load_wav(wav), dtype=np.float32)\n",
    "        w = self.load_wav(wav)\n",
    "\n",
    "        mel = self.ap.melspectrogram(w).astype('float32')\n",
    "\n",
    "        mel_lengths = mel.shape[1]\n",
    "\n",
    "\n",
    "        # mel = prepare_tensor(mel, 1)\n",
    "        mel = mel.transpose(1, 0)\n",
    "\n",
    "\n",
    "        # log mel  \n",
    "#         mel = np.log(mel)\n",
    "\n",
    "        # pad mel\n",
    "        if(mel_lengths >= self.pad_to):\n",
    "            mel = mel[:self.pad_to, :]\n",
    "        else:\n",
    "            N = self.pad_to - mel_lengths\n",
    "            zeros = np.ones((N,80))*self.pad_value\n",
    "            mel = np.concatenate((mel, zeros), axis = 0)\n",
    "\n",
    "        # print(mel.shape)\n",
    "\n",
    "\n",
    "        mel = torch.FloatTensor(mel).contiguous()\n",
    "        # mel_lengths = torch.LongTensor(mel_lengths)\n",
    "\n",
    "\n",
    "        # return {'wav': w, 'emotion': np.asarray(emotion)}\n",
    "        return {'mel': mel, 'emotion': emotion}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "\n",
    "        # w = batch[:,0]\n",
    "        # emotion = batch[:,1]\n",
    "        # print(w, emotion)\n",
    "        print(batch)\n",
    "\n",
    "        mel = [self.ap.melspectrogram(w).astype(\"float32\") for w in batch['wav']]\n",
    "        # mel = self.ap.melspectrogram(w).astype('float32')\n",
    "        mel_lengths = mel.shape[1]\n",
    "\n",
    "        mel = prepare_tensor(mel, self.outputs_per_step)\n",
    "        mel = mel.transpose(0, 2, 1)\n",
    "\n",
    "        mel = torch.FloatTensor(mel).contiguous()\n",
    "        mel_lengths = torch.LongTensor(mel_lengths)\n",
    "\n",
    "        return {'melspec': mel, 'mel_len': mel_lengths, 'emotion': torch.tensor(batch['emotion'])}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.load_data(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class NShotMAMLSampler(object):\n",
    "    '''\n",
    "        NShotMAMLSampler returns batches of meta samples\n",
    "        in a N-samples way of each class \n",
    "    '''\n",
    "\n",
    "    def __init__(self, targets, n_way, k_shot, lang, n_episodes_per_iter, shuffle, shuffle_once, include_query):\n",
    "        '''\n",
    "          n_way: number of examples for each of the k classes\n",
    "        '''\n",
    "        self.targets = targets\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.language = lang\n",
    "        self.batch_size = n_way*k_shot\n",
    "        self.episodes = n_episodes_per_iter\n",
    "        self.shuffle = shuffle\n",
    "        self.shuffle_once = shuffle_once\n",
    "        self.include_query = include_query\n",
    "\n",
    "        # Organize samples per target value\n",
    "        self.classes = torch.unique(self.targets).tolist()\n",
    "        self.num_classes = len(self.classes)\n",
    "\n",
    "        self.idx_per_class = {}\n",
    "        self.batches_per_class = {}\n",
    "\n",
    "        for c in self.classes:\n",
    "          self.idx_per_class[c] = torch.where(self.targets == c)[0]\n",
    "          self.batches_per_class[c] = self.idx_per_class[c].shape[0]//self.k_shot\n",
    "\n",
    "\n",
    "        # Create a list of classes from which we select the N classes per batch\n",
    "        self.iterations = sum(self.batches_per_class.values()) // self.n_way\n",
    "        self.class_list = [c for c in self.classes for _ in range(self.batches_per_class[c])]\n",
    "        if shuffle_once or self.shuffle:\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            # For testing, we iterate over classes instead of shuffling them\n",
    "            sort_idxs = [i+p*self.num_classes for i,\n",
    "                         c in enumerate(self.classes) for p in range(self.batches_per_class[c])]\n",
    "            self.class_list = np.array(self.class_list)[np.argsort(sort_idxs)].tolist()\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        # Shuffle the examples per class\n",
    "        for c in self.classes:\n",
    "            perm = torch.randperm(self.idx_per_class[c].shape[0])\n",
    "            self.idx_per_class[c] = self.idx_per_class[c][perm]\n",
    "        # Shuffle the class list from which we sample. Note that this way of shuffling\n",
    "        # does not prevent to choose the same class twice in a batch. However, for\n",
    "        # training and validation, this is not a problem.\n",
    "        random.shuffle(self.class_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle data\n",
    "        if self.shuffle:\n",
    "            self.shuffle_data()\n",
    "\n",
    "        # Sample few-shot batches\n",
    "        start_index = defaultdict(int)\n",
    "        for it in range(self.iterations):\n",
    "            class_batch = self.class_list[it*self.n_way:(it+1)*self.n_way]  # Select N classes for the batch\n",
    "            index_batch = []\n",
    "            for c in class_batch:  # For each class, select the next K examples and add them to the batch\n",
    "                index_batch.extend(self.idx_per_class[c][start_index[c]:start_index[c]+self.k_shot])\n",
    "                start_index[c] += self.k_shot\n",
    "            if self.include_query:  # If we return support+query set, sort them so that they are easy to split\n",
    "                index_batch = index_batch[::2] + index_batch[1::2]\n",
    "            yield index_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations\n",
    "      \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating debug test csv to read from debug_wavs path\n",
    "\n",
    "wav_path = \"../data/debug_wavs/\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.listdir(wav_path)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs = []\n",
    "emotions = []\n",
    "\n",
    "for i, wav in enumerate(os.listdir(wav_path)):\n",
    "    wavs.append(wav_path + wav)\n",
    "    emotions.append(i%6)#imod2 is a series of 0, 1, 2 and 3 just to simulate different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'wav_path': wavs, 'emotion': emotions})\n",
    "df.to_csv(\"df.csv\", index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loader\n",
    "N_WAY = 5\n",
    "K_SHOT = 5\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "\n",
    "ap = AudioProcessor(fft_size = 1024,\n",
    "                    hop_length = 256,\n",
    "                    win_length = 1024,\n",
    "                    pad_wav=False,\n",
    "                    num_mels = 80,\n",
    "                    mel_fmin = 80,\n",
    "                    mel_fmax = 7600,\n",
    "                    sample_rate = 22050,\n",
    "                    duration = None,\n",
    "                    resample = True,\n",
    "                    signal_norm= True,\n",
    "                    ref_level_db = 20,\n",
    "                    min_level_db = -100,\n",
    "                    symetric_norm = True,\n",
    "                    max_norm = 4)\n",
    "\n",
    "dataset = Dataloader4SER('df.csv', ap, pad_to = 250, pad_value = -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_data(1)['mel'].detach().cpu().numpy()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_wav(dataset.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_data(1)['mel'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load_data(0)['mel'].detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.imshow(dataset.load_data(1)['mel'].detach().cpu().numpy().T, origin = 'lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(dataset.load_data(1)['mel'].squeeze().detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ok the dataset loading melspectrogra with custom padding and padding values is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets dev the similar to Omniglot loader\n",
    "\n",
    "class SERNShot:\n",
    "    \n",
    "    def __init__(self, df_train_path, df_test_path, ap, batch_size, n_way, k_shot, k_query, pad_to = 200, pad_value = -3):\n",
    "        \n",
    "        self.df_train_path = df_train_path\n",
    "        self.df_test_path = df_test_path\n",
    "        self.ap = ap\n",
    "        self.pad_to = pad_to\n",
    "        self.pad_value = pad_value\n",
    "        self.batchsz = batch_size\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        \n",
    "        # Used for training, only 2 languages\n",
    "        self.dataset_train = Dataloader4SER(self.df_train_path, self.ap, pad_to = self.pad_to, pad_value = self.pad_value)\n",
    "        # Used for fine-tuning, the out-of-distribution language\n",
    "        self.dataset_test = Dataloader4SER(self.df_test_path, self.ap, pad_to = self.pad_to, pad_value = self.pad_value)\n",
    "        \n",
    "        self.length = self.pad_to\n",
    "        self.mel_dim = self.ap.num_mels\n",
    "        \n",
    "        x = self.dataset_train.x\n",
    "        y = self.dataset_train.y\n",
    "        \n",
    "        x_test = self.dataset_test.x\n",
    "        y_test = self.dataset_test.y\n",
    "        \n",
    "        temp = dict()\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            if(y[i] in temp.keys()):\n",
    "                temp[y[i]].append(self.dataset_train.load_data(i)['mel'].detach().numpy())\n",
    "            else:\n",
    "                temp[y[i]] = [self.dataset_train.load_data(i)['mel'].detach().numpy()]\n",
    "                \n",
    "        self.data = [] # It will be a [classes, mspec] array\n",
    "        for label, mspec in temp.items():\n",
    "            self.data.append(np.array(mspec))\n",
    "        \n",
    "        temp = dict()\n",
    "        \n",
    "        for i in range(len(x_test)):\n",
    "            if(y_test[i] in temp.keys()):\n",
    "                temp[y_test[i]].append(self.dataset_test.load_data(i)['mel'].detach().numpy())\n",
    "            else:\n",
    "                temp[y_test[i]] = [self.dataset_test.load_data(i)['mel'].detach().numpy()]\n",
    "                \n",
    "        self.test_data = [] # It will be a [classes, mspec] array\n",
    "        for label, mspec in temp.items():\n",
    "            self.test_data.append(np.array(mspec))\n",
    "        \n",
    "        self.data = np.array(self.data)\n",
    "        self.test_data = np.array(self.test_data)\n",
    "        \n",
    "        # Verbose\n",
    "        print(f\"There are {len(self.data)} samples for training and {len(self.test_data)} for out-of-distribution language.\")\n",
    "        \n",
    "        \n",
    "        self.n_cls_train = self.data.shape[0]\n",
    "        self.n_cls_test = self.test_data.shape[0]\n",
    "        \n",
    "        # Verbose \n",
    "        print(f\"n_class train = {self.n_cls_train} | n_class test = {self.n_cls_test}\")\n",
    "        \n",
    "        # Pointer of current read batch\n",
    "        self.indexes = {'train': 0, 'test': 0}\n",
    "        self.datasets = {'train': self.data, 'test': self.test_data}\n",
    "        \n",
    "        self.datasets_cache = {\"train\": self.load_data_cache(self.datasets[\"train\"]),  # current epoch data cached\n",
    "                               \"test\": self.load_data_cache(self.datasets[\"test\"])}\n",
    "        \n",
    "    def load_data_cache(self, data_pack):\n",
    "        \"\"\"\n",
    "        Collects several batches data for N-shot learning\n",
    "        :param data_pack: [cls_num, 20, 84, 84, 1]\n",
    "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "        \"\"\"\n",
    "        #  take 5 way 1 shot as example: 5 * 1\n",
    "        setsz = self.k_shot * self.n_way\n",
    "        querysz = self.k_query * self.n_way\n",
    "        data_cache = []\n",
    "\n",
    "        # print('preload next 50 caches of batchsz of batch.')\n",
    "        for sample in range(10):  # num of episodes\n",
    "\n",
    "            x_spts, y_spts, x_qrys, y_qrys = [], [], [], []\n",
    "            for i in range(self.batchsz):  # one batch means one set\n",
    "\n",
    "                x_spt, y_spt, x_qry, y_qry = [], [], [], []\n",
    "                selected_cls = np.random.choice(data_pack.shape[0], self.n_way, False)\n",
    "\n",
    "                for j, cur_class in enumerate(selected_cls):\n",
    "                    \n",
    "                    # Get min size over all classes as our min_sample to random select k_shot + k_query examples\n",
    "                    min_samples = np.min([s.shape[0] for s in data_pack])\n",
    "                    selected_img = np.random.choice(min_samples, self.k_shot + self.k_query, False)\n",
    "\n",
    "                    # meta-training and meta-test\n",
    "                    x_spt.append(data_pack[cur_class][selected_img[:self.k_shot]])\n",
    "                    x_qry.append(data_pack[cur_class][selected_img[self.k_shot:]])\n",
    "                    y_spt.append([j for _ in range(self.k_shot)])\n",
    "                    y_qry.append([j for _ in range(self.k_query)])\n",
    "\n",
    "                # shuffle inside a batch\n",
    "                perm = np.random.permutation(self.n_way * self.k_shot)\n",
    "#                 x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 1, self.resize, self.resize)[perm]\n",
    "                x_spt = np.array(x_spt).reshape(self.n_way * self.k_shot, 1, self.length, self.mel_dim)[perm]\n",
    "                y_spt = np.array(y_spt).reshape(self.n_way * self.k_shot)[perm]\n",
    "                perm = np.random.permutation(self.n_way * self.k_query)\n",
    "#                 x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 1, self.resize, self.resize)[perm]\n",
    "                x_qry = np.array(x_qry).reshape(self.n_way * self.k_query, 1, self.length, self.mel_dim)[perm]\n",
    "                y_qry = np.array(y_qry).reshape(self.n_way * self.k_query)[perm]\n",
    "\n",
    "                # append [sptsz, 1, 84, 84] => [b, setsz, 1, 84, 84]\n",
    "                x_spts.append(x_spt)\n",
    "                y_spts.append(y_spt)\n",
    "                x_qrys.append(x_qry)\n",
    "                y_qrys.append(y_qry)\n",
    "\n",
    "\n",
    "            # [b, setsz, 1, 84, 84]\n",
    "#             x_spts = np.array(x_spts).astype(np.float32).reshape(self.batchsz, setsz, 1, self.resize, self.resize)\n",
    "            x_spts = np.array(x_spts).astype(np.float32).reshape(self.batchsz, setsz, 1, self.length, self.mel_dim)\n",
    "            y_spts = np.array(y_spts).astype(np.int).reshape(self.batchsz, setsz)\n",
    "            # [b, qrysz, 1, 84, 84]\n",
    "#             x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, 1, self.resize, self.resize)\n",
    "            x_qrys = np.array(x_qrys).astype(np.float32).reshape(self.batchsz, querysz, self.length, self.mel_dim)\n",
    "            y_qrys = np.array(y_qrys).astype(np.int).reshape(self.batchsz, querysz)\n",
    "\n",
    "            data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "        return data_cache\n",
    "\n",
    "    def next(self, mode='train'):\n",
    "        \"\"\"\n",
    "        Gets next batch from the dataset with name.\n",
    "        :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # update cache if indexes is larger cached num\n",
    "        if self.indexes[mode] >= len(self.datasets_cache[mode]):\n",
    "            self.indexes[mode] = 0\n",
    "            self.datasets_cache[mode] = self.load_data_cache(self.datasets[mode])\n",
    "\n",
    "        next_batch = self.datasets_cache[mode][self.indexes[mode]]\n",
    "        self.indexes[mode] += 1\n",
    "\n",
    "        return next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nshot = SERNShot(df_train_path = 'df.csv', df_test_path = 'df.csv', ap = ap, batch_size = 2, n_way = 5, k_shot = 1, k_query = 1, pad_to = 200, pad_value = -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spt, y_spt, x_qry, y_qry = nshot.next('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spt , y_qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_spt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for v in nshot.data:\n",
    "    size_1 = v.shape[0]\n",
    "    sizes.append(size_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m_audio",
   "language": "python",
   "name": "m_audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
